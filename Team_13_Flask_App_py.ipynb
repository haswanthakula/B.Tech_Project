{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV2kPwYcYz1z",
        "outputId": "b0c15ede-7d14-443e-e62a-d5b341ce7c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, render_template, request, redirect, url_for\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from werkzeug.utils import secure_filename\n",
        "from itertools import combinations\n",
        "import itertools\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('apriori.html')\n",
        "\n",
        "###### machine learning code || Apriori\n",
        "#load dataset\n",
        "def load_data_set(file_path):\n",
        "    data_set = []\n",
        "    with open(file_path, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            cleaned_row = [value.strip() for value in row if value.strip()]  # Remove empty cells\n",
        "            if cleaned_row:  # Check if the row still has values after removing empty cells\n",
        "                data_set.append(cleaned_row)\n",
        "    return data_set\n",
        "\n",
        "#Frequent item set creation\n",
        "def create_C1(data_set):\n",
        "    C1 = set()\n",
        "    for t in data_set:\n",
        "        for item in t:\n",
        "            item_set = frozenset([item])\n",
        "            C1.add(item_set)\n",
        "    return  C1\n",
        "\n",
        "#check apriori\n",
        "def is_apriori(Ck_item, Lksub1):\n",
        "    for item in Ck_item:\n",
        "        sub_Ck = Ck_item - frozenset([item])\n",
        "        if sub_Ck not in Lksub1:\n",
        "            return  False\n",
        "    return True\n",
        "\n",
        "#Frequent k-itemset creation\n",
        "def create_Ck(Lksub1, k):\n",
        "    Ck = set()\n",
        "    len_Lksub1  =  len ( Lksub1 )\n",
        "    list_Lksub1 = list(Lksub1)\n",
        "    for  i  in  range ( len_Lksub1 ):\n",
        "        for  j  in  range ( 1 , len_Lksub1 ):\n",
        "            l1 = list(list_Lksub1[i])\n",
        "            l2 = list(list_Lksub1[j])\n",
        "            l1.sort()\n",
        "            l2.sort()\n",
        "            if  l1 [ 0 : k - 2 ] ==  l2 [ 0 : k - 2 ]:\n",
        "                Ck_item = list_Lksub1[i] | list_Lksub1[j]\n",
        "                # pruning\n",
        "                if is_apriori(Ck_item, Lksub1):\n",
        "                    Ck.add(Ck_item)\n",
        "    return Ck\n",
        "\n",
        "def generate_Lk_by_Ck(data_set, Ck, min_support, support_data):\n",
        "    Lk = set()\n",
        "    item_count = {}\n",
        "    for t in data_set:\n",
        "        for item in Ck:\n",
        "            if item.issubset(t):\n",
        "                if item not in item_count:\n",
        "                    item_count[item] = 1\n",
        "                else:\n",
        "                    item_count[item] += 1\n",
        "    t_num = float(len(data_set))\n",
        "    for item in item_count:\n",
        "        if ((item_count[item] / t_num) >= min_support):\n",
        "            Lk.add(item)\n",
        "            support_data[item] = item_count[item] / t_num\n",
        "    return Lk\n",
        "\n",
        "def generate_L(data_set, k, min_support):\n",
        "    support_data = {}\n",
        "    C1 = create_C1(data_set)\n",
        "    L1 = generate_Lk_by_Ck(data_set, C1, min_support, support_data)\n",
        "    Lksub1 = L1.copy()\n",
        "    L = []\n",
        "    L.append(Lksub1)\n",
        "    for  i  in  range ( 2 , k + 1 ):\n",
        "        Ci = create_Ck(Lksub1, i)\n",
        "        Li = generate_Lk_by_Ck(data_set, Ci, min_support, support_data)\n",
        "        Lksub1 = Li.copy()\n",
        "        L.append(Lksub1)\n",
        "    return L, support_data\n",
        "\n",
        "def generate_big_rules(L, support_data, min_conf):\n",
        "    big_rule_list = []\n",
        "    sub_set_list = []\n",
        "    for  i  in  range ( 0 , len ( L )):\n",
        "        for  freq_set  in  L [ i ]:\n",
        "            for sub_set in sub_set_list:\n",
        "                if sub_set.issubset(freq_set):\n",
        "                    conf = support_data[freq_set] / support_data[freq_set - sub_set]\n",
        "                    big_rule = (freq_set - sub_set, sub_set, conf)\n",
        "                    if conf >= min_conf and big_rule not in big_rule_list:\n",
        "                        #print(freq_set-sub_set, \" => \", sub_set, \"conf: \", conf)\n",
        "                        big_rule_list.append(big_rule)\n",
        "            sub_set_list.append(freq_set)\n",
        "    return big_rule_list\n",
        "\n",
        "\n",
        "###### machine learning code || Fp-Growth\n",
        "def find_uniItems(transactions):\n",
        "    unique_items = []\n",
        "    for i in transactions:\n",
        "        for j in i:\n",
        "            if j not in unique_items:\n",
        "                unique_items.append(j)\n",
        "    return unique_items\n",
        "\n",
        "def find_frequency(lists):\n",
        "    result = {}\n",
        "    for sub_list in lists:\n",
        "        sub_list_counter = Counter(sub_list)\n",
        "        for item, count in sub_list_counter.items():\n",
        "            if item in result:\n",
        "                result[item] += count\n",
        "            else:\n",
        "                result[item] = count\n",
        "    return result\n",
        "\n",
        "def remove_infrequent_and_sort(frequent_item_sets, min_support):\n",
        "    temp_itemset = frequent_item_sets.copy()\n",
        "    for key,values in frequent_item_sets.items():\n",
        "        if values < min_support:\n",
        "            temp_itemset.pop(key)\n",
        "        elif key == '':\n",
        "            temp_itemset.pop(key)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    frequent_item_sets = temp_itemset\n",
        "    keys = list(frequent_item_sets.keys())\n",
        "    values = list(frequent_item_sets.values())\n",
        "    sorted_value_index = np.argsort(values)\n",
        "    sorted_value_index = np.flip(sorted_value_index)\n",
        "    frequent_item_sets = {keys[i]: values[i] for i in sorted_value_index}\n",
        "    return frequent_item_sets\n",
        "\n",
        "def build_ordered_itemset(transactions, frequent_item_sets):\n",
        "    keys = list(frequent_item_sets.keys())\n",
        "    temp_transactions = []\n",
        "    for transaction in transactions:\n",
        "        temp_items = []\n",
        "        for item in transaction:\n",
        "            if item in keys:\n",
        "                temp_items.append(item)\n",
        "        temp_transactions.append(temp_items)\n",
        "\n",
        "    transactions = []\n",
        "    for temp_transaction in temp_transactions:\n",
        "        new_transaction = []\n",
        "        for key in keys:\n",
        "            if key in temp_transaction:\n",
        "                new_transaction.append(key)\n",
        "        transactions.append(new_transaction)\n",
        "\n",
        "    return transactions\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, item, count, parent):\n",
        "        self.item = item\n",
        "        self.count = count\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.next_node = None\n",
        "        self.link = None\n",
        "\n",
        "    def add_child(self, child):\n",
        "        if child.item not in self.children:\n",
        "            self.children[child.item] = child\n",
        "\n",
        "    def increment_count(self, count):\n",
        "        self.count += count\n",
        "\n",
        "    def get_nodes_with_item(self, item):\n",
        "        nodes = []\n",
        "        if self.item == item:\n",
        "            nodes.append(self)\n",
        "        for child in self.children.values():\n",
        "            nodes.extend(child.get_nodes_with_item(item))\n",
        "        return nodes\n",
        "\n",
        "class FPTree:\n",
        "    def __init__(self):\n",
        "        self.root = Node(\"*\", 0, None)\n",
        "        self.header_table = {}\n",
        "\n",
        "    def add_transaction(self, transaction):\n",
        "        current_node = self.root\n",
        "        for item in transaction:\n",
        "            child_node = current_node.children.get(item)\n",
        "            if child_node is None:\n",
        "                child_node = Node(item, 0, current_node)\n",
        "                current_node.children[item] = child_node\n",
        "                if item in self.header_table:\n",
        "                    last_node = self.header_table[item]\n",
        "                    while last_node.link is not None:\n",
        "                        last_node = last_node.link\n",
        "                    last_node.link = child_node\n",
        "                else:\n",
        "                    self.header_table[item] = child_node\n",
        "            child_node.increment_count(1)\n",
        "            current_node = child_node\n",
        "\n",
        "    def get_frequent_items(self, min_support):\n",
        "        frequent_items = {}\n",
        "        for item in self.header_table:\n",
        "            support = 0\n",
        "            node = self.header_table[item]\n",
        "            while node is not None:\n",
        "                support += node.count\n",
        "                node = node.link\n",
        "            if support >= min_support:\n",
        "                frequent_items[item] = support\n",
        "        return frequent_items\n",
        "\n",
        "    def get_nodes_with_item(self, item):\n",
        "        return self.header_table.get(item, [])\n",
        "\n",
        "def find_pattern_base(fptree, node, item):\n",
        "    pattern_base = {}\n",
        "    while node is not None:\n",
        "        prefix_path = []\n",
        "        temp_node = node\n",
        "        while temp_node.parent is not None:\n",
        "            if temp_node.name != \"null\" and temp_node.name != item:\n",
        "                prefix_path.append(temp_node.name)\n",
        "            temp_node = temp_node.parent\n",
        "        if len(prefix_path) > 0:\n",
        "            pattern_base[frozenset(prefix_path)] = node.count\n",
        "        if node.link is None:\n",
        "            break\n",
        "        node = node.link\n",
        "    return pattern_base\n",
        "\n",
        "def create_subtree(fptree, min_support):\n",
        "    items = list(fptree.header_table.keys())\n",
        "    for item in items:\n",
        "        support = 0\n",
        "        node = fptree.header_table[item]\n",
        "        while node is not None:\n",
        "            support += node.count\n",
        "            node = node.link\n",
        "        if support < min_support:\n",
        "            del fptree.header_table[item]\n",
        "        else:\n",
        "            fptree.header_table[item] = support\n",
        "    for item in fptree.header_table:\n",
        "        nodes = []\n",
        "        node = fptree.header_table[item]\n",
        "        while node is not None:\n",
        "            nodes.append(node)\n",
        "            node = node.link\n",
        "        fptree.header_table[item] = nodes\n",
        "    conditional_tree = FPTree()\n",
        "    for item in items:\n",
        "        pattern_base = find_pattern_base(fptree.root, item)\n",
        "        for transaction, count in pattern_base.items():\n",
        "            transaction_list = list(transaction)\n",
        "            for i in range(count):\n",
        "                conditional_tree.add_transaction(transaction_list)\n",
        "    frequent_items = conditional_tree.get_frequent_items(min_support)\n",
        "    for item in frequent_items:\n",
        "        conditional_tree.header_table[item] = frequent_items[item]\n",
        "    return conditional_tree\n",
        "\n",
        "def generate_frequent_patterns(fptree, min_support, prefix=[]):\n",
        "    items = [v[0] for v in sorted(fptree.items(), key=lambda kv: kv[1]['support'])]\n",
        "    for item in items:\n",
        "        new_prefix = prefix.copy()\n",
        "        new_prefix.append(item)\n",
        "        support = fptree[item][\"support\"]\n",
        "        yield (new_prefix, support)\n",
        "        conditional_pattern_base = find_pattern_base(fptree[item][\"node_link\"], item)\n",
        "        conditional_tree = create_subtree(conditional_pattern_base, min_support)\n",
        "        if len(conditional_tree) > 0:\n",
        "            for pattern in generate_frequent_patterns(conditional_tree, min_support, new_prefix):\n",
        "                yield pattern\n",
        "\n",
        "class FP_Growth:\n",
        "    def __init__(self, transactions, min_support):\n",
        "        self.transactions = transactions\n",
        "        self.min_support = min_support\n",
        "\n",
        "    def build_fptree(self):\n",
        "        self.fptree = FPTree()\n",
        "        for transaction in self.transactions:\n",
        "            self.fptree.add_transaction(transaction)\n",
        "\n",
        "    def generate_frequent_itemsets(self, node, suffix):\n",
        "        frequent_itemsets = []\n",
        "        support = node.count\n",
        "        for item, child_node in node.children.items():\n",
        "            itemset = suffix.copy()\n",
        "            itemset.add(item)\n",
        "            frequent_itemsets.append((itemset, support))\n",
        "            frequent_itemsets.extend(self.generate_frequent_itemsets(child_node, itemset))\n",
        "        return frequent_itemsets\n",
        "\n",
        "    def mine_frequent_itemsets(self):\n",
        "        self.build_fptree()\n",
        "        frequent_itemsets = []\n",
        "        for item, count in self.fptree.get_frequent_items(self.min_support).items():\n",
        "            frequent_itemsets.append((frozenset([item]), count))\n",
        "        for itemset in itertools.chain.from_iterable(\n",
        "                self.generate_frequent_itemsets(self.fptree.header_table[item], set()) for item in self.fptree.header_table):\n",
        "            frequent_itemsets.append(itemset)\n",
        "        return frequent_itemsets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_combinations(items, length):\n",
        "    return list(combinations(items, length))\n",
        "\n",
        "def generate_support(data, combination):\n",
        "    count = 0\n",
        "    for transaction in data:\n",
        "        if set(combination).issubset(set(transaction)):\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def eclat_2(data, min_support):\n",
        "    items = set()\n",
        "    for transaction in data:\n",
        "        for item in transaction:\n",
        "            items.add(item)\n",
        "\n",
        "    items = list(items)\n",
        "    items.sort()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    length = 1\n",
        "    while length <= len(items):\n",
        "        combinations = generate_combinations(items, length)\n",
        "        for combination in combinations:\n",
        "            support = generate_support(data, combination)\n",
        "            if support >= min_support:\n",
        "                results[tuple(combination)] = support\n",
        "        length += 1\n",
        "\n",
        "    return results\n",
        "\n",
        "@app.route('/apriori_algo',methods=[\"GET\",\"POST\"])\n",
        "def apriori_algo():\n",
        "    if request.method == 'POST':\n",
        "        starting_time = time.time()\n",
        "        file = request.files['file']\n",
        "        min_support = request.form.get('min_support')\n",
        "        min_conf = request.form.get('min_conf')\n",
        "        min_support = float(min_support)\n",
        "        min_conf = float(min_conf)\n",
        "        i_index = 1\n",
        "        web_apriori_output = [['frequent itemsets', 'support']]\n",
        "        web_apriori_strong_association_output = [['Item 1', 'Item 2', 'Confidence']]\n",
        "\n",
        "\n",
        "        # Save the file to ./uploads\n",
        "        basepath = os.path.dirname(__file__)\n",
        "        file_path = os.path.join(\n",
        "            basepath, 'uploads', secure_filename(file.filename))\n",
        "        file.save(file_path)\n",
        "\n",
        "        data_set = load_data_set(file_path)\n",
        "\n",
        "        L, support_data = generate_L(data_set, k=3, min_support = min_support)\n",
        "        for i, Lk in enumerate(L):\n",
        "            if len(Lk) > 0:\n",
        "                print(\"=\" * 50)\n",
        "                print(f\"frequent {len(list(Lk)[0])}-itemsets (frequent itemsets) \\t \\t support (support)\")\n",
        "                print(\"=\" * 50)\n",
        "                for freq_set in Lk:\n",
        "                    min_support_value = int(support_data[freq_set]*len(data_set))\n",
        "                    print(freq_set, min_support_value)\n",
        "                    web_apriori_output.insert(i_index, [freq_set, min_support_value])\n",
        "                    i_index = i_index+1\n",
        "                else:\n",
        "                    print(f\"No frequent {i+1}-itemsets found.\")\n",
        "\n",
        "        big_rules_list = generate_big_rules(L, support_data, min_conf=min_conf)\n",
        "        if(len(big_rules_list)>0):\n",
        "            print ( \"Strong Association\\n\" )\n",
        "            i_index = 1\n",
        "            for item in big_rules_list:\n",
        "                print ( item [ 0 ], \"=>\" , item [ 1 ], \"conf(confidence): \" , item [ 2 ])\n",
        "                web_apriori_strong_association_output.insert(i_index, [item [ 0 ],item [ 1 ],item [ 2 ]])\n",
        "                i_index = i_index+1\n",
        "        else:\n",
        "            print ( \"No Strong Association are found!\" )\n",
        "\n",
        "        with open(\"web_apriori_strong_association_output.csv\", \"w+\") as my_csv:\n",
        "            csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "            csvWriter.writerows(web_apriori_strong_association_output)\n",
        "        df = pd.read_csv(\"web_apriori_strong_association_output.csv\")\n",
        "\n",
        "        with open(\"web_apriori_output.csv\", \"w+\") as my_csv:\n",
        "            csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "            csvWriter.writerows(web_apriori_output)\n",
        "        apriori_algo_csv = pd.read_csv(\"web_apriori_output.csv\")\n",
        "        end_time = time.time()\n",
        "\n",
        "        exe_time = end_time - starting_time\n",
        "        exe_time = round(exe_time, 4)\n",
        "\n",
        "    return render_template('apriori.html',tables=[df.to_html(classes='data')], tables1=[apriori_algo_csv.to_html(classes='data')], execution_time = exe_time)\n",
        "\n",
        "@app.route('/fpgrowth_algo',methods=[\"GET\",\"POST\"])\n",
        "def fpgrowth_algo():\n",
        "    if request.method == 'POST':\n",
        "        starting_time = time.time()\n",
        "        file = request.files['file']\n",
        "        min_support = request.form.get('min_support')\n",
        "        ref = float(min_support)\n",
        "        i_index = 1\n",
        "        web_fpgrowth_output = [['frequent itemsets', 'support']]\n",
        "\n",
        "        # Save the file to ./uploads\n",
        "        basepath = os.path.dirname(__file__)\n",
        "        file_path = os.path.join(\n",
        "            basepath, 'uploads', secure_filename(file.filename))\n",
        "        file.save(file_path)\n",
        "\n",
        "        dataset = load_data_set(file_path)\n",
        "        min_support_1 = ref *len(dataset)\n",
        "        unique_items = find_uniItems(dataset)\n",
        "        frequent_item_sets = find_frequency(dataset)\n",
        "        frequent_item_sets = remove_infrequent_and_sort(frequent_item_sets, min_support_1)\n",
        "        transactions = build_ordered_itemset(dataset, frequent_item_sets)\n",
        "\n",
        "        # root, item_counts = construct_tree(dataset, min_support)\n",
        "        # frequent_itemsets = []\n",
        "        # fp_growth(root, item_counts, [], min_support, frequent_itemsets)\n",
        "        fp = FP_Growth(transactions, min_support_1)\n",
        "        frequent_itemsets = fp.mine_frequent_itemsets()\n",
        "\n",
        "\n",
        "        print(\"Frequent Itemsets | Support\")\n",
        "        for itemset, support in frequent_itemsets:\n",
        "            if(support > min_support_1):\n",
        "                print(itemset, support)\n",
        "                web_fpgrowth_output.insert(i_index, [itemset, support])\n",
        "                i_index = i_index+1\n",
        "\n",
        "        with open(\"web_fpgrowth_output.csv\", \"w+\") as my_csv:\n",
        "            csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "            csvWriter.writerows(web_fpgrowth_output)\n",
        "        df = pd.read_csv(\"web_fpgrowth_output.csv\")\n",
        "        end_time = time.time()\n",
        "\n",
        "        exe_time = end_time - starting_time\n",
        "        print(exe_time)\n",
        "        exe_time = round(exe_time, 4)\n",
        "\n",
        "    return render_template('fpgrowth.html',tables=[df.to_html(classes='data')], execution_time = exe_time)\n",
        "\n",
        "\n",
        "@app.route('/apriori')\n",
        "def movie():\n",
        "    return render_template('apriori.html')\n",
        "\n",
        "@app.route('/fpgrowth')\n",
        "def fpgrowth():\n",
        "    return render_template('fpgrowth.html')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)\n"
      ]
    }
  ]
}